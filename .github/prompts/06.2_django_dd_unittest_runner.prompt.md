---
description: Execute unit tests and generate comprehensive test reports based on tests created in prompt 06.1
---

# Django Design Detail (DD) Unit Test Runner & Reporter

This prompt executes the unit tests generated in prompt 06.1 and creates comprehensive test reports with coverage analysis, performance metrics, and actionable recommendations.

## Prerequisites

**Required Input:**
- 📁 **Generated Tests**: Output from `06.1_django_dd_unittest_writer.prompt.md`
- 📦 **Project Name**: `${input:project_name}`
- 🏷️ **App Name**: `${input:app_name}`
- 📄 **Test Summary Path**: `test_generation/generated_tests_summary.json`

## Test Execution Workflow

### Step 1: Setup Test Environment

**Prepare test environment and dependencies:**

```bash
echo "=== SETTING UP TEST ENVIRONMENT ==="

# Create test reports directory
mkdir -p test_reports/{coverage,performance,detailed}

# Check if tests were generated
if [ ! -d "tests/test_${input:app_name}" ]; then
    echo "❌ No tests found. Run prompt 06.1 first to generate tests."
    exit 1
fi

echo "✅ Test directory found: tests/test_${input:app_name}/"

# Check test database setup
echo "Setting up test database..."
python manage.py migrate --run-syncdb --settings=config.settings.test 2>/dev/null || \
python manage.py migrate --run-syncdb

echo "✅ Test database ready"
```

**Install test dependencies:**

```bash
# Install testing tools if not present
echo "Installing test dependencies..."

pip install coverage pytest-django pytest-cov pytest-xdist pytest-html 2>/dev/null || echo "Dependencies already installed"

echo "✅ Test dependencies ready"
```

### Step 2: Execute Unit Tests with Coverage

**Run comprehensive test suite with coverage analysis:**

```python
# File: test_execution/test_runner.py

"""
Comprehensive test runner with coverage and performance analysis
"""

import subprocess
import json
import time
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any

class ComprehensiveTestRunner:
    """Run tests and generate comprehensive reports"""
    
    def __init__(self, app_name: str):
        self.app_name = app_name
        self.test_results = {
            'execution_metadata': {
                'app_name': app_name,
                'start_time': datetime.now().isoformat(),
                'end_time': None,
                'total_duration': 0
            },
            'test_execution': {},
            'coverage_analysis': {},
            'performance_metrics': {},
            'failure_analysis': {},
            'recommendations': []
        }
    
    def run_basic_tests(self) -> Dict:
        """Run basic unit tests"""
        print("Running basic unit tests...")
        
        start_time = time.time()
        
        # Run Django tests
        result = subprocess.run(
            [
                'python', 'manage.py', 'test', 
                f'tests.test_{self.app_name}',
                '--verbosity=2',
                '--keepdb'
            ],
            capture_output=True,
            text=True,
            timeout=600  # 10 minute timeout
        )
        
        end_time = time.time()
        duration = end_time - start_time
        
        # Parse test output
        test_results = self._parse_test_output(result.stdout, result.stderr)
        test_results['duration'] = duration
        test_results['return_code'] = result.returncode
        test_results['success'] = result.returncode == 0
        
        return test_results
    
    def run_coverage_analysis(self) -> Dict:
        """Run tests with coverage analysis"""
        print("Running coverage analysis...")
        
        # Run tests with coverage
        coverage_result = subprocess.run(
            [
                'coverage', 'run', '--source=apps', 
                'manage.py', 'test', f'tests.test_{self.app_name}',
                '--keepdb'
            ],
            capture_output=True,
            text=True,
            timeout=600
        )
        
        # Generate coverage report
        report_result = subprocess.run(
            ['coverage', 'report', '--format=text'],
            capture_output=True,
            text=True
        )
        
        # Generate JSON coverage report
        json_result = subprocess.run(
            ['coverage', 'json', '-o', 'test_reports/coverage/coverage.json'],
            capture_output=True,
            text=True
        )
        
        # Generate HTML coverage report
        html_result = subprocess.run(
            ['coverage', 'html', '-d', 'test_reports/coverage/html'],
            capture_output=True,
            text=True
        )
        
        coverage_data = {
            'coverage_run_success': coverage_result.returncode == 0,
            'report_generation_success': report_result.returncode == 0,
            'coverage_text': report_result.stdout,
            'coverage_data': self._load_coverage_json(),
            'html_report_generated': html_result.returncode == 0
        }
        
        return coverage_data
    
    def run_performance_tests(self) -> Dict:
        """Run performance-focused tests"""
        print("Running performance analysis...")
        
        performance_data = {
            'test_execution_times': {},
            'slow_tests': [],
            'memory_usage': {},
            'database_queries': {}
        }
        
        # Run tests with timing
        timing_result = subprocess.run(
            [
                'python', 'manage.py', 'test', 
                f'tests.test_{self.app_name}',
                '--verbosity=2',
                '--timing',
                '--keepdb'
            ],
            capture_output=True,
            text=True,
            timeout=600
        )
        
        # Parse timing information
        performance_data['test_execution_times'] = self._parse_test_timing(timing_result.stdout)
        
        # Identify slow tests (>1 second)
        slow_tests = []
        for test_name, duration in performance_data['test_execution_times'].items():
            if duration > 1.0:
                slow_tests.append({
                    'test_name': test_name,
                    'duration': duration,
                    'recommendation': 'Consider optimization or mocking external dependencies'
                })
        
        performance_data['slow_tests'] = slow_tests
        
        return performance_data
    
    def analyze_test_failures(self) -> Dict:
        """Analyze test failures and provide recommendations"""
        print("Analyzing test failures...")
        
        failure_analysis = {
            'failed_tests': [],
            'error_categories': {},
            'common_patterns': [],
            'fix_recommendations': []
        }
        
        # Run tests and capture detailed failure information
        detailed_result = subprocess.run(
            [
                'python', 'manage.py', 'test', 
                f'tests.test_{self.app_name}',
                '--verbosity=3',
                '--failfast=False',
                '--keepdb'
            ],
            capture_output=True,
            text=True,
            timeout=600
        )
        
        if detailed_result.returncode != 0:
            # Parse failure details
            failures = self._parse_test_failures(detailed_result.stdout, detailed_result.stderr)
            failure_analysis['failed_tests'] = failures
            
            # Categorize errors
            error_categories = {}
            for failure in failures:
                error_type = self._categorize_error(failure['error_message'])
                if error_type not in error_categories:
                    error_categories[error_type] = []
                error_categories[error_type].append(failure)
            
            failure_analysis['error_categories'] = error_categories
            
            # Generate recommendations
            recommendations = self._generate_failure_recommendations(error_categories)
            failure_analysis['fix_recommendations'] = recommendations
        
        return failure_analysis
    
    def run_specific_test_suites(self) -> Dict:
        """Run specific test suites separately"""
        print("Running specific test suites...")
        
        suite_results = {}
        
        test_files = [
            'test_models',
            'test_serializers', 
            'test_views',
            'test_api',
            'test_business_logic'
        ]
        
        for test_file in test_files:
            test_path = f'tests.test_{self.app_name}.{test_file}'
            
            print(f"  Running {test_file}...")
            
            result = subprocess.run(
                [
                    'python', 'manage.py', 'test', 
                    test_path,
                    '--verbosity=2',
                    '--keepdb'
                ],
                capture_output=True,
                text=True,
                timeout=300
            )
            
            suite_results[test_file] = {
                'success': result.returncode == 0,
                'stdout': result.stdout,
                'stderr': result.stderr,
                'test_count': self._count_tests_in_output(result.stdout)
            }
        
        return suite_results
    
    def _parse_test_output(self, stdout: str, stderr: str) -> Dict:
        """Parse Django test runner output"""
        lines = stdout.split('\n')
        
        test_info = {
            'tests_run': 0,
            'failures': 0,
            'errors': 0,
            'skipped': 0,
            'test_details': []
        }
        
        # Look for test summary line
        for line in lines:
            if 'Ran ' in line and ' test' in line:
                # Extract number of tests run
                parts = line.split()
                if len(parts) >= 2:
                    try:
                        test_info['tests_run'] = int(parts[1])
                    except ValueError:
                        pass
            
            elif 'FAILED (' in line:
                # Parse failure details
                if 'failures=' in line:
                    failures = line.split('failures=')[1].split(',')[0].split(')')[0]
                    try:
                        test_info['failures'] = int(failures)
                    except ValueError:
                        pass
                
                if 'errors=' in line:
                    errors = line.split('errors=')[1].split(',')[0].split(')')[0]
                    try:
                        test_info['errors'] = int(errors)
                    except ValueError:
                        pass
        
        # Calculate success rate
        total_tests = test_info['tests_run']
        failed_tests = test_info['failures'] + test_info['errors']
        
        if total_tests > 0:
            test_info['success_rate'] = ((total_tests - failed_tests) / total_tests) * 100
        else:
            test_info['success_rate'] = 0
        
        return test_info
    
    def _load_coverage_json(self) -> Dict:
        """Load coverage data from JSON file"""
        coverage_file = Path('test_reports/coverage/coverage.json')
        
        if coverage_file.exists():
            with open(coverage_file, 'r') as f:
                return json.load(f)
        
        return {}
    
    def _parse_test_timing(self, output: str) -> Dict:
        """Parse test timing information"""
        timing_data = {}
        
        # This is a simplified parser - actual implementation would need
        # more sophisticated parsing based on Django test runner output
        lines = output.split('\n')
        
        for line in lines:
            if ' ... ' in line and 'ok' in line:
                # Simple timing extraction (would need actual timing data)
                test_name = line.split(' ... ')[0].strip()
                # Default timing placeholder
                timing_data[test_name] = 0.1
        
        return timing_data
    
    def _parse_test_failures(self, stdout: str, stderr: str) -> List[Dict]:
        """Parse test failure details"""
        failures = []
        
        lines = (stdout + stderr).split('\n')
        current_failure = None
        
        for line in lines:
            if 'FAIL:' in line or 'ERROR:' in line:
                if current_failure:
                    failures.append(current_failure)
                
                current_failure = {
                    'test_name': line.split(': ')[1] if ': ' in line else 'Unknown',
                    'failure_type': 'FAIL' if 'FAIL:' in line else 'ERROR',
                    'error_message': '',
                    'traceback': []
                }
            
            elif current_failure and line.strip():
                if line.startswith('  '):
                    current_failure['traceback'].append(line)
                else:
                    current_failure['error_message'] += line + '\n'
        
        if current_failure:
            failures.append(current_failure)
        
        return failures
    
    def _categorize_error(self, error_message: str) -> str:
        """Categorize error based on error message"""
        error_lower = error_message.lower()
        
        if 'attributeerror' in error_lower:
            return 'attribute_error'
        elif 'assertionerror' in error_lower:
            return 'assertion_failure'
        elif 'validationerror' in error_lower:
            return 'validation_error'
        elif 'integrityerror' in error_lower:
            return 'database_integrity'
        elif 'importerror' in error_lower or 'modulenotfounderror' in error_lower:
            return 'import_error'
        elif 'permission' in error_lower:
            return 'permission_error'
        else:
            return 'other'
    
    def _generate_failure_recommendations(self, error_categories: Dict) -> List[str]:
        """Generate recommendations based on error categories"""
        recommendations = []
        
        if 'import_error' in error_categories:
            recommendations.append("Check import statements and ensure all required modules are installed")
        
        if 'assertion_failure' in error_categories:
            recommendations.append("Review test assertions and expected vs actual values")
        
        if 'validation_error' in error_categories:
            recommendations.append("Verify model field validations and test data")
        
        if 'database_integrity' in error_categories:
            recommendations.append("Check database constraints and ensure test data meets requirements")
        
        if 'permission_error' in error_categories:
            recommendations.append("Verify user permissions and authentication in test setup")
        
        return recommendations
    
    def _count_tests_in_output(self, output: str) -> int:
        """Count number of tests from output"""
        lines = output.split('\n')
        
        for line in lines:
            if 'Ran ' in line and ' test' in line:
                parts = line.split()
                if len(parts) >= 2:
                    try:
                        return int(parts[1])
                    except ValueError:
                        pass
        
        return 0
    
    def run_comprehensive_tests(self) -> Dict:
        """Run comprehensive test suite"""
        print("=== RUNNING COMPREHENSIVE TEST SUITE ===")
        
        start_time = time.time()
        
        # Basic test execution
        self.test_results['test_execution'] = self.run_basic_tests()
        
        # Coverage analysis
        self.test_results['coverage_analysis'] = self.run_coverage_analysis()
        
        # Performance analysis
        self.test_results['performance_metrics'] = self.run_performance_tests()
        
        # Failure analysis
        self.test_results['failure_analysis'] = self.analyze_test_failures()
        
        # Suite-specific results
        self.test_results['suite_results'] = self.run_specific_test_suites()
        
        end_time = time.time()
        total_duration = end_time - start_time
        
        # Update metadata
        self.test_results['execution_metadata']['end_time'] = datetime.now().isoformat()
        self.test_results['execution_metadata']['total_duration'] = total_duration
        
        # Generate overall recommendations
        self.test_results['recommendations'] = self._generate_overall_recommendations()
        
        return self.test_results
    
    def _generate_overall_recommendations(self) -> List[str]:
        """Generate overall recommendations based on all test results"""
        recommendations = []
        
        # Test execution recommendations
        test_exec = self.test_results.get('test_execution', {})
        success_rate = test_exec.get('success_rate', 0)
        
        if success_rate < 80:
            recommendations.append("Test success rate is below 80%. Focus on fixing failing tests before proceeding.")
        elif success_rate < 95:
            recommendations.append("Test success rate is good but could be improved. Review failing tests.")
        else:
            recommendations.append("Excellent test success rate! Tests are well-implemented.")
        
        # Coverage recommendations
        coverage = self.test_results.get('coverage_analysis', {}).get('coverage_data', {})
        if coverage:
            totals = coverage.get('totals', {})
            coverage_percent = totals.get('percent_covered', 0)
            
            if coverage_percent < 70:
                recommendations.append("Test coverage is below 70%. Add more unit tests to improve coverage.")
            elif coverage_percent < 90:
                recommendations.append("Test coverage is good but could be improved. Focus on untested code paths.")
            else:
                recommendations.append("Excellent test coverage! Well done on comprehensive testing.")
        
        # Performance recommendations
        slow_tests = self.test_results.get('performance_metrics', {}).get('slow_tests', [])
        if slow_tests:
            recommendations.append(f"Found {len(slow_tests)} slow tests. Consider optimization or mocking.")
        
        # Failure-specific recommendations
        failure_recs = self.test_results.get('failure_analysis', {}).get('fix_recommendations', [])
        recommendations.extend(failure_recs)
        
        return recommendations


def run_comprehensive_test_suite():
    """Main function to run comprehensive test suite"""
    runner = ComprehensiveTestRunner('${input:app_name}')
    results = runner.run_comprehensive_tests()
    
    # Save detailed results
    with open('test_reports/detailed/test_execution_results.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    return results


if __name__ == '__main__':
    run_comprehensive_test_suite()
```

### Step 3: Generate Test Reports

**Create comprehensive test reports in multiple formats:**

```python
# File: test_execution/test_reporter.py

"""
Generate comprehensive test reports
"""

import json
from datetime import datetime
from pathlib import Path
from typing import Dict, List

class TestReporter:
    """Generate comprehensive test reports"""
    
    def __init__(self, test_results: Dict):
        self.test_results = test_results
        self.app_name = test_results['execution_metadata']['app_name']
    
    def generate_summary_report(self) -> str:
        """Generate executive summary report"""
        
        exec_metadata = self.test_results['execution_metadata']
        test_exec = self.test_results['test_execution']
        coverage = self.test_results['coverage_analysis'].get('coverage_data', {})
        
        # Calculate overall score
        success_rate = test_exec.get('success_rate', 0)
        coverage_percent = coverage.get('totals', {}).get('percent_covered', 0)
        overall_score = (success_rate * 0.6) + (coverage_percent * 0.4)
        
        summary = f"""
=== TEST EXECUTION SUMMARY REPORT ===

App: {self.app_name}
Execution Date: {exec_metadata['start_time']}
Total Duration: {exec_metadata['total_duration']:.2f} seconds

OVERALL SCORE: {overall_score:.1f}/100

TEST EXECUTION RESULTS:
✅ Tests Run: {test_exec.get('tests_run', 0)}
✅ Passed: {test_exec.get('tests_run', 0) - test_exec.get('failures', 0) - test_exec.get('errors', 0)}
❌ Failed: {test_exec.get('failures', 0)}
⚠️ Errors: {test_exec.get('errors', 0)}
📊 Success Rate: {success_rate:.1f}%

COVERAGE ANALYSIS:
📊 Overall Coverage: {coverage_percent:.1f}%
📁 Files Covered: {len(coverage.get('files', {}))}
📝 Lines Covered: {coverage.get('totals', {}).get('covered_lines', 0)}/{coverage.get('totals', {}).get('num_statements', 0)}

PERFORMANCE METRICS:
⚡ Slow Tests: {len(self.test_results.get('performance_metrics', {}).get('slow_tests', []))}
⏱️ Average Test Duration: {exec_metadata['total_duration'] / max(test_exec.get('tests_run', 1), 1):.3f}s

RECOMMENDATIONS:
"""
        
        for i, rec in enumerate(self.test_results.get('recommendations', []), 1):
            summary += f"{i}. {rec}\n"
        
        return summary
    
    def generate_detailed_html_report(self) -> str:
        """Generate detailed HTML report"""
        
        exec_metadata = self.test_results['execution_metadata']
        test_exec = self.test_results['test_execution']
        coverage = self.test_results['coverage_analysis'].get('coverage_data', {})
        suite_results = self.test_results.get('suite_results', {})
        
        html_report = f"""
<!DOCTYPE html>
<html>
<head>
    <title>Test Report - {self.app_name}</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        .header {{ background: #f0f0f0; padding: 20px; margin-bottom: 20px; border-radius: 5px; }}
        .summary {{ display: flex; gap: 20px; margin-bottom: 30px; }}
        .metric-box {{ padding: 15px; border: 1px solid #ddd; border-radius: 5px; text-align: center; flex: 1; }}
        .success {{ background: #d4edda; border-color: #c3e6cb; }}
        .warning {{ background: #fff3cd; border-color: #ffeaa7; }}
        .error {{ background: #f8d7da; border-color: #f5c6cb; }}
        .section {{ margin: 30px 0; }}
        .test-file {{ margin: 20px 0; padding: 15px; border: 1px solid #eee; }}
        .recommendations {{ background: #e3f2fd; padding: 20px; border-radius: 5px; }}
        table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
        th, td {{ padding: 10px; text-align: left; border-bottom: 1px solid #ddd; }}
        th {{ background-color: #f5f5f5; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>Test Execution Report: {self.app_name}</h1>
        <p><strong>Generated:</strong> {exec_metadata['start_time']}</p>
        <p><strong>Duration:</strong> {exec_metadata['total_duration']:.2f} seconds</p>
    </div>
    
    <div class="summary">
        <div class="metric-box {'success' if test_exec.get('success_rate', 0) >= 90 else 'warning' if test_exec.get('success_rate', 0) >= 70 else 'error'}">
            <h3>{test_exec.get('success_rate', 0):.1f}%</h3>
            <p>Success Rate</p>
        </div>
        <div class="metric-box">
            <h3>{test_exec.get('tests_run', 0)}</h3>
            <p>Tests Run</p>
        </div>
        <div class="metric-box {'success' if coverage.get('totals', {}).get('percent_covered', 0) >= 80 else 'warning'}">
            <h3>{coverage.get('totals', {}).get('percent_covered', 0):.1f}%</h3>
            <p>Coverage</p>
        </div>
        <div class="metric-box {'success' if test_exec.get('failures', 0) == 0 else 'error'}">
            <h3>{test_exec.get('failures', 0)}</h3>
            <p>Failures</p>
        </div>
    </div>
    
    <div class="section">
        <h2>Test Suite Results</h2>
        <table>
            <tr>
                <th>Test Suite</th>
                <th>Status</th>
                <th>Tests</th>
                <th>Notes</th>
            </tr>
"""
        
        for suite_name, suite_data in suite_results.items():
            status = "✅ Pass" if suite_data.get('success') else "❌ Fail"
            test_count = suite_data.get('test_count', 0)
            
            html_report += f"""
            <tr>
                <td>{suite_name.replace('_', ' ').title()}</td>
                <td>{status}</td>
                <td>{test_count}</td>
                <td>{'All tests passed' if suite_data.get('success') else 'Some tests failed'}</td>
            </tr>
"""
        
        html_report += """
        </table>
    </div>
"""
        
        # Coverage details
        if coverage.get('files'):
            html_report += """
    <div class="section">
        <h2>Coverage by File</h2>
        <table>
            <tr>
                <th>File</th>
                <th>Coverage</th>
                <th>Lines</th>
                <th>Missing</th>
            </tr>
"""
            
            for file_path, file_data in coverage.get('files', {}).items():
                if f'apps/{self.app_name}' in file_path:
                    coverage_percent = file_data.get('summary', {}).get('percent_covered', 0)
                    covered_lines = file_data.get('summary', {}).get('covered_lines', 0)
                    total_lines = file_data.get('summary', {}).get('num_statements', 0)
                    missing_lines = file_data.get('missing_lines', [])
                    
                    html_report += f"""
            <tr>
                <td>{file_path.split('/')[-1]}</td>
                <td>{coverage_percent:.1f}%</td>
                <td>{covered_lines}/{total_lines}</td>
                <td>{len(missing_lines)} lines</td>
            </tr>
"""
            
            html_report += """
        </table>
    </div>
"""
        
        # Recommendations
        recommendations = self.test_results.get('recommendations', [])
        if recommendations:
            html_report += """
    <div class="section">
        <h2>Recommendations</h2>
        <div class="recommendations">
            <ul>
"""
            
            for rec in recommendations:
                html_report += f"                <li>{rec}</li>\n"
            
            html_report += """
            </ul>
        </div>
    </div>
"""
        
        html_report += """
</body>
</html>
"""
        
        return html_report
    
    def generate_json_report(self) -> Dict:
        """Generate machine-readable JSON report"""
        
        exec_metadata = self.test_results['execution_metadata']
        test_exec = self.test_results['test_execution']
        coverage = self.test_results['coverage_analysis'].get('coverage_data', {})
        
        # Calculate quality metrics
        success_rate = test_exec.get('success_rate', 0)
        coverage_percent = coverage.get('totals', {}).get('percent_covered', 0)
        
        quality_score = {
            'overall_score': (success_rate * 0.6) + (coverage_percent * 0.4),
            'test_quality': success_rate,
            'coverage_quality': coverage_percent,
            'grade': self._calculate_grade((success_rate * 0.6) + (coverage_percent * 0.4))
        }
        
        json_report = {
            'report_metadata': {
                'app_name': self.app_name,
                'generated_at': datetime.now().isoformat(),
                'execution_duration': exec_metadata['total_duration']
            },
            'quality_metrics': quality_score,
            'test_execution_summary': {
                'total_tests': test_exec.get('tests_run', 0),
                'passed_tests': test_exec.get('tests_run', 0) - test_exec.get('failures', 0) - test_exec.get('errors', 0),
                'failed_tests': test_exec.get('failures', 0),
                'error_tests': test_exec.get('errors', 0),
                'success_rate': success_rate
            },
            'coverage_summary': {
                'overall_coverage': coverage_percent,
                'files_covered': len(coverage.get('files', {})),
                'lines_covered': coverage.get('totals', {}).get('covered_lines', 0),
                'total_lines': coverage.get('totals', {}).get('num_statements', 0)
            },
            'performance_metrics': {
                'slow_tests_count': len(self.test_results.get('performance_metrics', {}).get('slow_tests', [])),
                'average_test_duration': exec_metadata['total_duration'] / max(test_exec.get('tests_run', 1), 1)
            },
            'recommendations': self.test_results.get('recommendations', []),
            'detailed_results': self.test_results
        }
        
        return json_report
    
    def _calculate_grade(self, score: float) -> str:
        """Calculate letter grade based on score"""
        if score >= 95:
            return 'A+'
        elif score >= 90:
            return 'A'
        elif score >= 85:
            return 'B+'
        elif score >= 80:
            return 'B'
        elif score >= 75:
            return 'C+'
        elif score >= 70:
            return 'C'
        elif score >= 60:
            return 'D'
        else:
            return 'F'
    
    def generate_all_reports(self):
        """Generate all report formats"""
        print("Generating comprehensive test reports...")
        
        # Text summary report
        summary = self.generate_summary_report()
        with open('test_reports/test_summary.txt', 'w') as f:
            f.write(summary)
        
        # HTML detailed report
        html_report = self.generate_detailed_html_report()
        with open('test_reports/detailed/test_report.html', 'w') as f:
            f.write(html_report)
        
        # JSON report
        json_report = self.generate_json_report()
        with open('test_reports/test_report.json', 'w') as f:
            json.dump(json_report, f, indent=2)
        
        print("✅ Test reports generated:")
        print("   📄 Text Summary: test_reports/test_summary.txt")
        print("   🌐 HTML Report: test_reports/detailed/test_report.html")
        print("   📊 JSON Report: test_reports/test_report.json")
        print("   📊 Coverage HTML: test_reports/coverage/html/index.html")
        
        # Print summary to console
        print(summary)
        
        return {
            'summary_report': summary,
            'json_report': json_report,
            'html_report_path': 'test_reports/detailed/test_report.html'
        }


def generate_test_reports():
    """Main function to generate test reports"""
    
    # Load test execution results
    with open('test_reports/detailed/test_execution_results.json', 'r') as f:
        test_results = json.load(f)
    
    reporter = TestReporter(test_results)
    reports = reporter.generate_all_reports()
    
    return reports


if __name__ == '__main__':
    generate_test_reports()
```

### Step 4: Performance and Quality Analysis

**Analyze test performance and code quality:**

```bash
# Performance and quality analysis
echo "=== PERFORMANCE AND QUALITY ANALYSIS ==="

# Check for test performance issues
echo "Analyzing test performance..."

# Run tests with profiling
python -m cProfile -o test_reports/performance/test_profile.prof manage.py test tests.test_${input:app_name} --keepdb

# Generate performance report
echo "import pstats; p = pstats.Stats('test_reports/performance/test_profile.prof'); p.sort_stats('cumulative').print_stats(20)" | python

# Check for code quality issues in tests
echo "Analyzing test code quality..."

# Run flake8 on test files
flake8 tests/test_${input:app_name}/ --output-file=test_reports/quality/test_code_quality.txt --statistics || true

# Check test complexity
echo "Test complexity analysis..."
find tests/test_${input:app_name}/ -name "*.py" -exec wc -l {} \; | sort -nr > test_reports/quality/test_file_sizes.txt

echo "✅ Performance and quality analysis complete"
```

## Main Test Execution Script

**Execute the complete test runner workflow:**

```bash
#!/bin/bash

# Main test execution script
echo "=== DJANGO DD UNIT TEST RUNNER ==="

# Check prerequisites
if [ ! -f "test_generation/generated_tests_summary.json" ]; then
    echo "❌ No test generation summary found. Run prompt 06.1 first."
    exit 1
fi

# Setup environment
echo "Setting up test environment..."
mkdir -p test_reports/{coverage,performance,detailed,quality}

# Install dependencies
pip install coverage pytest-django pytest-html pytest-cov 2>/dev/null || true

# Run comprehensive test suite
echo "Running comprehensive test suite..."
python test_execution/test_runner.py

# Generate reports
echo "Generating test reports..."
python test_execution/test_reporter.py

# Run performance analysis
echo "Running performance analysis..."
python -m cProfile -o test_reports/performance/test_profile.prof manage.py test tests.test_${input:app_name} --keepdb 2>/dev/null || true

# Quality analysis
echo "Running quality analysis..."
flake8 tests/test_${input:app_name}/ --output-file=test_reports/quality/test_code_quality.txt --statistics 2>/dev/null || true

echo ""
echo "=== TEST EXECUTION COMPLETE ==="
echo "📊 Summary: test_reports/test_summary.txt"
echo "🌐 HTML Report: test_reports/detailed/test_report.html"
echo "📄 JSON Report: test_reports/test_report.json"
echo "📊 Coverage Report: test_reports/coverage/html/index.html"
echo ""
echo "Open the HTML report in your browser to view detailed results."
```

## Success Criteria

**Test execution is successful when:**

✅ **All generated tests are executed successfully**  
✅ **Comprehensive test reports are generated**  
✅ **Coverage analysis shows adequate coverage (>70%)**  
✅ **Performance metrics are within acceptable ranges**  
✅ **Failure analysis provides actionable recommendations**  
✅ **Multiple report formats are available (text, HTML, JSON)**  
✅ **Quality scores and grades are calculated**

---

**Note**: This prompt executes the tests generated in prompt 06.1 and provides comprehensive analysis and reporting. Use the reports to improve test quality and coverage.